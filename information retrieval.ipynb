{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0cc0a3",
   "metadata": {},
   "source": [
    "# crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ba1be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "def get_html(url):\n",
    "    response = requests.get(url)\n",
    "    return response.content.decode('utf-8')\n",
    "\n",
    "def extract_links(html, base_url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = set()\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href is not None:\n",
    "            abs_url = urljoin(base_url, href)\n",
    "            links.add(abs_url)\n",
    "    return links\n",
    "\n",
    "def crawl(start_url, max_urls=30):\n",
    "    urls_to_crawl = {start_url}\n",
    "    crawled_urls = set()\n",
    "\n",
    "    while urls_to_crawl and len(crawled_urls) < max_urls:\n",
    "        url = urls_to_crawl.pop()\n",
    "        if url in crawled_urls:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            html = get_html(url)\n",
    "            links = extract_links(html, url)\n",
    "            crawled_urls.add(url)\n",
    "            urls_to_crawl.update(links)\n",
    "        except:\n",
    "            print(f\"Error crawling {url}\")\n",
    "    \n",
    "    return crawled_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ad3b38bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error crawling javascript:void(0);\n"
     ]
    }
   ],
   "source": [
    "crawlded=crawl(\"https://www.homeandlearn.co.uk/WD/wds5pA.html\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6d492",
   "metadata": {},
   "source": [
    "The get_html function simply sends a HTTP request to the given URL and returns the HTML content. \n",
    "\n",
    "The extract_links function uses BeautifulSoup to parse the HTML and extract all hyperlinks (i.e. a tags) in the document. It then converts any relative URLs to absolute URLs using the urljoin function, which takes care of handling any differences in the base URL. \n",
    "\n",
    "The crawl function implements a basic breadth-first search algorithm, starting from the start_url and recursively crawling any new URLs that are discovered. It keeps track of URLs that have already been crawled to avoid revisiting them. Finally, it returns the set of all crawled URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bc4ff",
   "metadata": {},
   "source": [
    "# Dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e43d67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_index_entries(urls):\n",
    "    index = defaultdict(list)\n",
    "    stop_words = set(stopwords.words('english')) # set of stop words\n",
    "    stemmer = PorterStemmer() # create stemmer object\n",
    "\n",
    "    for url in urls:\n",
    "        # Get the HTML code given the URL of a document\n",
    "        response = requests.get(url)\n",
    "        content = response.text\n",
    "\n",
    "        # Extract the text content of the document\n",
    "        text = extract_text(content)\n",
    "\n",
    "        # Apply text segmentation\n",
    "        sentences = segment_text(text)\n",
    "\n",
    "        # Apply text normalization (lowercase, lemmatization, token filtering)\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_text(sentence)\n",
    "            tokens = filter_tokens(tokens, stop_words)\n",
    "            tokens = lemmatize_stem_tokens(tokens, stemmer)\n",
    "            for token in tokens:\n",
    "                index[token].append(url)\n",
    "    del index['']\n",
    "    return index\n",
    "\n",
    "\n",
    "def extract_text(content):\n",
    "    # Use regular expressions to extract text content between HTML tags\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def segment_text(text):\n",
    "    # Use NLTK to segment text into sentences\n",
    "    sentences = word_tokenize(text.lower())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Use NLTK to tokenize text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def filter_tokens(tokens, stop_words):\n",
    "    # Filter out stop words and other unwanted tokens\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        # Filter out stop words\n",
    "        if token.lower() not in stop_words:\n",
    "            # Filter out certain characters\n",
    "            token = re.sub(r'[^\\w\\s]+', '', token)\n",
    "            # Filter out digits\n",
    "            if not token.isdigit():\n",
    "                # Add the filtered token to the list of filtered tokens\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def lemmatize_stem_tokens(tokens, stemmer):\n",
    "    # Use NLTK to lemmatize/stem tokens\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        # Lemmatize nouns\n",
    "        if 'NN' in nltk.pos_tag([token])[0][1]:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            normalized_token = lemmatizer.lemmatize(token)\n",
    "        # Stem verbs and other parts of speech\n",
    "        else:\n",
    "            normalized_token = stemmer.stem(token)\n",
    "        normalized_tokens.append(normalized_token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0a5ce",
   "metadata": {},
   "source": [
    "1-extract_index_entries(urls): This function takes in a list of URLs and returns a dictionary of index entries where the keys are words and the values are lists of URLs where the word appears.\n",
    "\n",
    "2-extract_text(content): This function takes in HTML content and uses BeautifulSoup to extract the text content of the document by removing any script and style tags.\n",
    "\n",
    "3-segment_text(text): This function takes in a string of text and uses the NLTK library to segment the text into sentences.\n",
    "\n",
    "4-tokenize_text(text): This function takes in a string of text and uses the NLTK library to tokenize the text into individual words.\n",
    "\n",
    "5-filter_tokens(tokens, stop_words): This function takes in a list of tokens and a set of stop words and filters out stop words and unwanted characters.\n",
    "\n",
    "6-lemmatize_stem_tokens(tokens, stemmer): This function takes in a list of tokens and a stemmer object and uses the NLTK library to lemmatize nouns and stem other parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "64713ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=extract_index_entries(crawlded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc7790",
   "metadata": {},
   "source": [
    "# Document indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e856ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(url):\n",
    "    all_words=[]\n",
    "    index = defaultdict(list)\n",
    "    stop_words = set(stopwords.words('english')) # set of stop words\n",
    "    stemmer = PorterStemmer() # create stemmer object\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "    # Extract the text content of the document\n",
    "    text = extract_text(content)\n",
    "    # Apply text segmentation\n",
    "    sentences = segment_text(text)\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize_text(sentence)\n",
    "        tokens = filter_tokens(tokens, stop_words)\n",
    "        tokens = lemmatize_stem_tokens(tokens, stemmer)            \n",
    "        for token in tokens:\n",
    "            all_words.append(token)\n",
    "    all_words= list(filter(lambda x: x != '', all_words))\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "65587069",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(crawlded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7649cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words=extract_content(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2bb6a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_indexing(dictionnary, content):\n",
    "\n",
    "    tf = Counter(content)\n",
    "\n",
    "    # initialize document representation\n",
    "    boolean_model = [0] * len(dictionnary)\n",
    "    tf_model = [0] * len(dictionnary)\n",
    "    wf_model = [0] * len(dictionnary)\n",
    "    tf_idf_model = [0] * len(dictionnary)\n",
    "    wf_idf_model = [0] * len(dictionnary)\n",
    "\n",
    "    # calculate document representation for each term\n",
    "    for i, term in enumerate(dictionnary):\n",
    "        # term occurrence (boolean model)\n",
    "        if term in content:\n",
    "            boolean_model[i] = 1\n",
    "\n",
    "        # term frequency tf\n",
    "        tf_model[i] = tf[term]\n",
    "\n",
    "        # term frequency wf=1+logtf if tf≠0 and 0 otherwise\n",
    "        if tf[term] != 0:\n",
    "            wf_model[i] = 1 + math.log(tf[term], 10)\n",
    "\n",
    "        # term frequency × inverse document frequency tf.idf\n",
    "        if term in tf and tf[term] != 0:\n",
    "            tf_idf_model[i] = tf[term] * math.log(len(dictionnary[term]) / len(dictionnary), 10)\n",
    "\n",
    "        # term frequency × inverse document frequency wf.idf\n",
    "        if tf[term] != 0:\n",
    "            idf = math.log(len(dictionnary[term]) / len(dictionnary), 10)\n",
    "            wf_idf_model[i] = (1 + math.log(tf[term], 10)) * idf\n",
    "\n",
    "    return {\n",
    "        'boolean_model': boolean_model,\n",
    "        'tf_model': tf_model,\n",
    "        'wf_model': wf_model,\n",
    "        'tf_idf_model': tf_idf_model,\n",
    "        'wf_idf_model': wf_idf_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "62965769",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_models=document_indexing(indices,my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8cb8d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=indices.keys(),index=my_models.keys())\n",
    "for model_name in my_models.keys():\n",
    "    model_values = my_models[model_name]\n",
    "    for i in range(len(indices.keys())):\n",
    "        df.loc[model_name, list(indices.keys())[i]] = model_values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a0f22e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>microsoft</th>\n",
       "      <th>excel</th>\n",
       "      <th>tutorial</th>\n",
       "      <th>copy</th>\n",
       "      <th>paste</th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "      <th>de</th>\n",
       "      <th>e</th>\n",
       "      <th>hr</th>\n",
       "      <th>...</th>\n",
       "      <th>sites2023</th>\n",
       "      <th>waymentions</th>\n",
       "      <th>légales</th>\n",
       "      <th>cgv</th>\n",
       "      <th>personnelles</th>\n",
       "      <th>intend</th>\n",
       "      <th>class1vb</th>\n",
       "      <th>convertpostcodevb</th>\n",
       "      <th>reuse</th>\n",
       "      <th>performs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>boolean_model</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_model</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wf_model</th>\n",
       "      <td>1.69897</td>\n",
       "      <td>2.041393</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.69897</td>\n",
       "      <td>1.90309</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_idf_model</th>\n",
       "      <td>-4.851424</td>\n",
       "      <td>-8.30209</td>\n",
       "      <td>-3.350169</td>\n",
       "      <td>-9.822663</td>\n",
       "      <td>-16.903619</td>\n",
       "      <td>-1.468769</td>\n",
       "      <td>-1.562045</td>\n",
       "      <td>-1.041889</td>\n",
       "      <td>-1.548681</td>\n",
       "      <td>-1.562045</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wf_idf_model</th>\n",
       "      <td>-1.648485</td>\n",
       "      <td>-1.540711</td>\n",
       "      <td>-2.179335</td>\n",
       "      <td>-3.337682</td>\n",
       "      <td>-4.021139</td>\n",
       "      <td>-1.468769</td>\n",
       "      <td>-1.562045</td>\n",
       "      <td>-1.041889</td>\n",
       "      <td>-1.548681</td>\n",
       "      <td>-1.562045</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              microsoft     excel  tutorial      copy      paste        en  \\\n",
       "boolean_model         1         1         1         1          1         1   \n",
       "tf_model              5        11         2         5          8         1   \n",
       "wf_model        1.69897  2.041393   1.30103   1.69897    1.90309       1.0   \n",
       "tf_idf_model  -4.851424  -8.30209 -3.350169 -9.822663 -16.903619 -1.468769   \n",
       "wf_idf_model  -1.648485 -1.540711 -2.179335 -3.337682  -4.021139 -1.468769   \n",
       "\n",
       "                     fr        de         e        hr  ... sites2023  \\\n",
       "boolean_model         1         1         1         1  ...         0   \n",
       "tf_model              1         1         1         1  ...         0   \n",
       "wf_model            1.0       1.0       1.0       1.0  ...         0   \n",
       "tf_idf_model  -1.562045 -1.041889 -1.548681 -1.562045  ...         0   \n",
       "wf_idf_model  -1.562045 -1.041889 -1.548681 -1.562045  ...         0   \n",
       "\n",
       "              waymentions légales cgv personnelles intend class1vb  \\\n",
       "boolean_model           0       0   0            0      0        0   \n",
       "tf_model                0       0   0            0      0        0   \n",
       "wf_model                0       0   0            0      0        0   \n",
       "tf_idf_model            0       0   0            0      0        0   \n",
       "wf_idf_model            0       0   0            0      0        0   \n",
       "\n",
       "              convertpostcodevb reuse performs  \n",
       "boolean_model                 0     0        0  \n",
       "tf_model                      0     0        0  \n",
       "wf_model                      0     0        0  \n",
       "tf_idf_model                  0     0        0  \n",
       "wf_idf_model                  0     0        0  \n",
       "\n",
       "[5 rows x 3502 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c32eb",
   "metadata": {},
   "source": [
    "now we have a function called document indexing, that when giving it a list of words appeared in a document, it will give us a dictionnary of the document represantation on each model (boolean, tf,wf,tf_idf,wf_idf)\n",
    "\n",
    "the above dictionnary is for the first link in the crawlded links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43599f6c",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a7738bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"i want to learn excel because i love excel\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "19b9a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_array=list(crawlded)\n",
    "urls_indexing={}\n",
    "for i in urls_array:\n",
    "    my_words=extract_content(i)\n",
    "    urls_indexing[i]=document_indexing(indices,my_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7be1c",
   "metadata": {},
   "source": [
    "here we are building a dictionnary where the keys are the urls and the values is a dictionnary of document indexing in all weighting schemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c84cc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_similarity(query,urls_indexing):\n",
    "    query_array=query.split(' ')\n",
    "    my_query_indexing=document_indexing(indices,query_array)\n",
    "    similarities={\n",
    "        'boolean_model': [],\n",
    "        'tf_model': [],\n",
    "        'wf_model': [],\n",
    "        'tf_idf_model': [],\n",
    "        'wf_idf_model': []\n",
    "    }\n",
    "    for j in my_query_indexing.keys():\n",
    "        for i in urls_indexing.keys():\n",
    "            similarities[j].append([i,cosine_similarity(my_query_indexing[j],urls_indexing[i][j])])\n",
    "    return similarities\n",
    "        \n",
    "            \n",
    "def cosine_similarity(d1, d2):\n",
    "    dot_product = np.dot(d1, d2)\n",
    "    norm1 = np.linalg.norm(d1)\n",
    "    norm2 = np.linalg.norm(d2)\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8da146aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_similarity=compute_similarity(query,urls_indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f43f72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in query_similarity.keys():\n",
    "    query_similarity[i]=sorted(query_similarity[i], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4fbde87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the boolean_model is suggesting https://www.homeandlearn.co.uk/php/php5p6.html\n",
      "the boolean_model is suggesting https://www.homeandlearn.co.uk/extras/image-information/add-windows-form-control-runtime.html\n",
      "the boolean_model is suggesting https://www.homeandlearn.co.uk/php/php2p9.html\n",
      "the boolean_model is suggesting https://www.homeandlearn.co.uk/games-programming/3d-games-programming.html\n",
      "the boolean_model is suggesting https://www.homeandlearn.co.uk/php/php4p4.html\n",
      "\n",
      "the tf_model is suggesting https://www.homeandlearn.co.uk/excel2007/Excel2007.html\n",
      "the tf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s1p1.html\n",
      "the tf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s7p2.html\n",
      "the tf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s7p4.html\n",
      "the tf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s8p1.html\n",
      "\n",
      "the wf_model is suggesting https://www.homeandlearn.co.uk/excel2007/roundup-excel.html\n",
      "the wf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s7p8.html\n",
      "the wf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s7p2.html\n",
      "the wf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s3p1.html\n",
      "the wf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s8p2.html\n",
      "\n",
      "the tf_idf_model is suggesting https://www.homeandlearn.co.uk/excel2007/Excel2007.html\n",
      "the tf_idf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s1p1.html\n",
      "the tf_idf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s7p2.html\n",
      "the tf_idf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s8p1.html\n",
      "the tf_idf_model is suggesting https://www.homeandlearn.co.uk/excel2007/excel2007s8p4.html\n",
      "\n",
      "the wf_idf_model is suggesting https://www.homeandlearn.co.uk/extras/image-information/add-windows-form-control-runtime.html\n",
      "the wf_idf_model is suggesting https://www.homeandlearn.co.uk/games-programming/3d-games-programming.html\n",
      "the wf_idf_model is suggesting https://www.homeandlearn.co.uk/NET/nets8p6.html\n",
      "the wf_idf_model is suggesting https://www.homeandlearn.co.uk/excel2007/roundup-excel.html\n",
      "the wf_idf_model is suggesting https://www.homeandlearn.co.uk/NET/nets8p5.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def suggest_links(query_similarity,max_suggested):\n",
    "    for key, value in query_similarity.items():\n",
    "        for v in range(max_suggested):\n",
    "            print(\"the {} is suggesting {}\".format(key, value[v][0]))\n",
    "        print()\n",
    "suggest_links(query_similarity,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6972b9a",
   "metadata": {},
   "source": [
    "as we can see, after giving the query \"i want to learn excel because i love excel\" each model suggests some links , but as we can see the boolean is not efficient, the tf and tf_idf and wf are the best since they are suggesting only excel pages, and wf_idf is  suggesting only one excel page"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
